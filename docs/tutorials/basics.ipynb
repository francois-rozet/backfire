{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics\n",
    "\n",
    "This notebook walks you through the basics of PyTorch/Zuko distributions and transformations, how to parametrize probabilistic models, how to instantiate pre-built normalizing flows and finally how to create custom flow architectures. Training is covered in other tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import zuko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributions and transformations\n",
    "\n",
    "PyTorch defines two components for probabilistic modeling: the [`Distribution`](torch.distributions.distribution.Distribution) and the [`Transform`](torch.distributions.transforms.Transform). A distribution object represents the probability distribution $p(X)$ of a random variable $X$. A distribution must implement the `sample` and `log_prob` methods, meaning that we can draw realizations $x \\sim p(X)$ from the distribution and evaluate the log-likelihood $\\log p(X = x)$ of realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2122), tensor(-0.9415))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = torch.distributions.Normal(torch.tensor(0.0), torch.tensor(1.0))\n",
    "\n",
    "x = distribution.sample()\n",
    "log_p = distribution.log_prob(x)\n",
    "\n",
    "x, log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transform object represents a bijective transformation $f: X \\mapsto Y$ from a domain to a co-domain. A transformation must implement a forward call $y = f(x)$, an inverse call $x = f^{-1}(y)$ and the `log_abs_det_jacobian` method to compute the log-absolute-determinant of the transfomation's Jacobian $\\log \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.6367), tensor(0.2122), tensor(1.0986))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = torch.distributions.AffineTransform(torch.tensor(2.0), torch.tensor(3.0))\n",
    "\n",
    "y = transform(x)\n",
    "x_ = transform.inv(y)\n",
    "ladj = transform.log_abs_det_jacobian(x, y)\n",
    "\n",
    "y, x_, ladj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining a base distribution $p(Z)$ and a transformation $f: X \\mapsto Z$ defines a new distribution $p(X)$. The likelihood is given by the change of variables formula\n",
    "\n",
    "$$ p(X = x) = p(Z = f(x)) \\left| \\det \\frac{\\partial f(x)}{\\partial x} \\right| $$\n",
    "\n",
    "and sampling from $p(X)$ can be performed by first drawing realizations $z \\sim p(Z)$ and then applying the inverse transformation $x = f^{-1}(z)$. Such combination of a base distribution and a bijective transformation is sometimes called a *normalizing flow* as the base distribution is often Gaussian (normal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.6321), tensor(0.1743))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow = zuko.distributions.NormalizingFlow(transform, distribution)\n",
    "\n",
    "x = flow.sample()\n",
    "log_p = flow.log_prob(x)\n",
    "\n",
    "x, log_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy parametrization\n",
    "\n",
    "When designing the distributions module, the PyTorch team decided that distributions and transformations should be lightweight objects that are used as part of computations but destroyed afterwards. Consequently, the [`Distribution`](torch.distributions.distribution.Distribution) and [`Transform`](torch.distributions.transforms.Transform) classes are not sub-classes of [`torch.nn.Module`](torch.nn.Module), which means that we cannot retrieve their parameters with `.parameters()`, send their internal tensor to GPU with `.to('cuda')` or train them as regular neural networks. In addition, the concepts of conditional distribution and transformation, which are essential for probabilistic inference, are impossible to express with the current interface.\n",
    "\n",
    "To solve these problems, [`zuko`](zuko) defines two concepts: the [`LazyDistribution`](zuko.flows.core.LazyDistribution) and the [`LazyTransform`](zuko.flows.core.LazyTransform), which are modules whose forward pass returns a distribution or transformation, respectively. These components hold the parameters of the distributions/transformations as well as the recipe to build them, such that the actual distribution/transformation objects are lazily built and destroyed when necessary. Importantly, because the creation of the distribution/transformation object is delayed, an eventual condition can be easily taken into account. This design enables lazy distributions to act like distributions while retaining features inherent to modules, such as trainable parameters.\n",
    "\n",
    "In the following cell, we define a simpe Gaussian model of the form $\\mathcal{N}(x | \\mu_\\phi(c), \\sigma_\\phi^2(c))$ where $c$ is a context vector and $\\phi$ are the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianModel(\n",
       "  (hyper): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GaussianModel(zuko.flows.LazyDistribution):\n",
    "    def __init__(self, context: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hyper = torch.nn.Sequential(\n",
    "            torch.nn.Linear(context, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 2), # mu, log sigma\n",
    "        )\n",
    "\n",
    "    def forward(self, c: torch.Tensor):\n",
    "        mu, log_sigma = self.hyper(c).unbind(dim=-1)\n",
    "        \n",
    "        return torch.distributions.Normal(mu, log_sigma.exp())\n",
    "\n",
    "model = GaussianModel(8)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the forward method of the model with a context $c$ returns a distribution object, which we can use to draw realizations or evaluate the likelihood of realizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normal(loc: -0.05041343718767166, scale: 1.0097345113754272)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = model(c=torch.randn(8))\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2154, -1.0755, -1.0740], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution.log_prob(torch.randn(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of `log_prob` is part of a computation graph (it has a `grad_fn`) and can be used to train the parameters of the model. Conversely, the result of `sample` is not part of a computation graph. If you need to train the parameters through sampling, you should use the `rsample` method instead, which is not implemented by all distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.6246, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution.rsample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, if you modify the parameters of the model, for example with gradient descent steps, you must always remember to call the forward method again to re-build the distribution with the new parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for _ in range(8):\n",
    "    c = torch.randn(8)\n",
    "    x = torch.normal(torch.sum(c), torch.prod(torch.abs(c)))\n",
    "\n",
    "    loss = -model(c).log_prob(x)  # -log p(x | c)\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing flows\n",
    "\n",
    "Following the same spirit, a normalizing flow in Zuko is a sepcial `LazyDistribution` that contains a `LazyTransform` and a base `LazyDistribution`. To increase expressivity, the transformation is usually the composition of a sequence of \"simple\" transformations. For example, a masked autoregressive flow (MAF) usually has a few transformations and a standard Gaussian base distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MAF(\n",
       "  (transform): LazyComposedTransform(\n",
       "    (0): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicAffineTransform()\n",
       "      (order): [0, 1, 2, 3, 4]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=13, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=128, out_features=256, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): MaskedLinear(in_features=256, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicAffineTransform()\n",
       "      (order): [4, 3, 2, 1, 0]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=13, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=128, out_features=256, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): MaskedLinear(in_features=256, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicAffineTransform()\n",
       "      (order): [0, 1, 2, 3, 4]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=13, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=128, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=128, out_features=256, bias=True)\n",
       "        (5): ReLU()\n",
       "        (6): MaskedLinear(in_features=256, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (base): Unconditional(DiagNormal(loc: torch.Size([5]), scale: torch.Size([5])))\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow = zuko.flows.MAF(features=5, context=8, transforms=3, hidden_features=(64, 128, 256))\n",
    "flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous cell, we instantiated a contitonal flow (3 sample features and 8 context features) with 3 affine autoregressive transformations, each parameterized by a masked MLP with increasing number of hidden neurons. Zuko provides many pre-built flow architectures including [`NICE`](zuko.flows.coupling.NICE), [`MAF`](zuko.flows.autoregressive.MAF), [`NSF`](zuko.flows.spline.NSF), [`CNF`](zuko.flows.continuous.CNF) and many others. We recommend users to try `MAF` and `NSF` first as they are efficient baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom architecture\n",
    "\n",
    "Alternatively, a flow can be built as a custom [`Flow`](zuko.flows.core.Flow) object given a sequence of lazy transformations and a base lazy distribution. Follows a condensed example of many things that are possible in Zuko. But remember, with great power comes great responsibility (and great bugs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flow(\n",
       "  (transform): LazyComposedTransform(\n",
       "    (0): Unconditional(AffineTransform())\n",
       "    (1): Unconditional(Inverse(SigmoidTransform()))\n",
       "    (2): MaskedAutoregressiveTransform(\n",
       "      (base): MonotonicAffineTransform()\n",
       "      (order): [0, 1, 2, 3, 4]\n",
       "      (hyper): MaskedMLP(\n",
       "        (0): MaskedLinear(in_features=13, out_features=64, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "        (3): ReLU()\n",
       "        (4): MaskedLinear(in_features=64, out_features=10, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): Unconditional(RotationTransform())\n",
       "    (4): LazyInverse(\n",
       "      (transform): GeneralCouplingTransform(\n",
       "        (base): MonotonicRQSTransform(bins=8)\n",
       "        (mask): [0, 1, 0, 1, 0]\n",
       "        (hyper): MLP(\n",
       "          (0): Linear(in_features=10, out_features=128, bias=True)\n",
       "          (1): ELU(alpha=1.0)\n",
       "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "          (3): ELU(alpha=1.0)\n",
       "          (4): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (5): ELU(alpha=1.0)\n",
       "          (6): Linear(in_features=512, out_features=69, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Unconditional(\n",
       "      (meta): NeuralAutoregressiveTransform(\n",
       "        (base): MonotonicTransform()\n",
       "        (order): [1, 0, 0, 1, 0]\n",
       "        (hyper): MaskedMLP(\n",
       "          (0): MaskedLinear(in_features=5, out_features=64, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): MaskedLinear(in_features=64, out_features=64, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): MaskedLinear(in_features=64, out_features=80, bias=True)\n",
       "        )\n",
       "        (network): MonotonicMLP(\n",
       "          (0): MonotonicLinear(in_features=17, out_features=64, bias=True, stack=5)\n",
       "          (1): TwoWayELU(alpha=1.0)\n",
       "          (2): MonotonicLinear(in_features=64, out_features=64, bias=True, stack=5)\n",
       "          (3): TwoWayELU(alpha=1.0)\n",
       "          (4): MonotonicLinear(in_features=64, out_features=1, bias=True, stack=5)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (base): Unconditional(BoxUniform(low: torch.Size([5]), high: torch.Size([5])))\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zuko.flows import (\n",
    "    Flow,\n",
    "    GeneralCouplingTransform,\n",
    "    MaskedAutoregressiveTransform,\n",
    "    NeuralAutoregressiveTransform,\n",
    "    Unconditional,\n",
    ")\n",
    "from zuko.distributions import BoxUniform\n",
    "from zuko.transforms import (\n",
    "    AffineTransform,\n",
    "    MonotonicRQSTransform,\n",
    "    RotationTransform,\n",
    "    SigmoidTransform,\n",
    ")\n",
    "\n",
    "flow = Flow(\n",
    "    transform=[\n",
    "        # Preprocessing\n",
    "        Unconditional(  # [0, 255] -> ]0, 1[\n",
    "            AffineTransform,  # y = loc + scale * x\n",
    "            torch.tensor(1 / 512),  # loc\n",
    "            torch.tensor(1 / 256),  # scale\n",
    "            buffer=True,  # not trainable\n",
    "        ),\n",
    "        Unconditional(lambda: SigmoidTransform().inv),  # y = logit(x)\n",
    "        # Transformations\n",
    "        MaskedAutoregressiveTransform(  # autoregressive transform (affine by default)\n",
    "            features=5,\n",
    "            context=8,\n",
    "            passes=5,  # fully-autoregressive\n",
    "            hidden_features=(64, 64),\n",
    "        ),\n",
    "        Unconditional(RotationTransform, torch.randn(5, 5)),  # trainable rotation\n",
    "        GeneralCouplingTransform(  # coupling transform\n",
    "            features=5,\n",
    "            context=8,\n",
    "            univariate=MonotonicRQSTransform,  # rational-quadratic spline\n",
    "            shapes=([8], [8], [7]),  # shapes of the spline parameters (8 bins)\n",
    "            hidden_features=(128, 256, 512),\n",
    "            activation=torch.nn.ELU,  # ELU activation in hyper-network\n",
    "        ).inv,  # inverse\n",
    "        Unconditional(  # ignore context\n",
    "            NeuralAutoregressiveTransform(  # neural autoregressive transform\n",
    "                features=5,\n",
    "                order=[5, 2, 0, 3, 1],  # autoregressive order\n",
    "                passes=2,  #  2-pass autoregressive (equivalent to coupling)\n",
    "            )\n",
    "        ),\n",
    "    ],\n",
    "    base=Unconditional(  # ignore context\n",
    "        BoxUniform,\n",
    "        torch.full((5,), -3.0),\n",
    "        torch.full((5,), +3.0),\n",
    "        buffer=True,  # not trainable\n",
    "    ),\n",
    ")\n",
    "\n",
    "flow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zuko",
   "language": "python",
   "name": "zuko"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
